import os
import glob

# Configuration
RULESET_DIR = os.path.join(os.path.dirname(__file__), "../ruleset/Surge(Shadowkroket)")

# Priority Definitions (Higher priority lists steal domains from Lower priority lists)
# Format: "Specific": ["Generic1", "Generic2"]
# Meaning: If a domain is in Specific, remove it from Generic1 and Generic2.
#
# Priority order (high to low):
#   1. Ad-blocking rulesets (AdBlock, NSFW) - Highest priority
#   2. Specific site rulesets (Twitter, Netflix, Steam, etc.) - Medium priority
#   3. Fallback rulesets (GlobalProxy, GlobalMedia, SocialMedia, etc.) - Lowest priority
#
CONFLICT_MAP = {
    # ========== First Priority: Ad-blocking ==========
    # AdBlock takes priority over all other rulesets
    "AdBlock.list": ["GlobalProxy.list", "SocialMedia.list", 
                     "Google.list", "Microsoft.list", "Apple.list",
                     "YouTube.list", "Spotify.list", "NSFW.list", "AI.list"],
    
    # ========== Second Priority: Specific site rulesets ==========
    # Social media specific (merged rulesets)
    "SocialMedia.list": ["GlobalProxy.list", "NSFW.list", "AI.list"],
    
    # Streaming specific (merged rulesets)
    "StreamUS.list": ["GlobalProxy.list", "NSFW.list"],
    "StreamTW.list": ["GlobalProxy.list", "NSFW.list"],
    "StreamHK.list": ["GlobalProxy.list", "NSFW.list"],
    "StreamJP.list": ["GlobalProxy.list", "NSFW.list"],
    "StreamKR.list": ["GlobalProxy.list", "NSFW.list"],
    "StreamEU.list": ["GlobalProxy.list", "NSFW.list"],
    "YouTube.list": ["GlobalProxy.list", "Google.list", "NSFW.list"],
    "Spotify.list": ["GlobalProxy.list", "NSFW.list"],
    "TikTok.list": ["SocialMedia.list", "GlobalProxy.list", "NSFW.list"],
    "Telegram.list": ["SocialMedia.list", "GlobalProxy.list", "NSFW.list"],
    
    # Gaming specific
    "Gaming.list": ["GlobalProxy.list", "NSFW.list"],
    "Steam.list": ["Gaming.list", "GlobalProxy.list", "NSFW.list"],
    "Epic.list": ["Gaming.list", "GlobalProxy.list", "NSFW.list"],
    
    # AI specific
    "AI.list": ["GlobalProxy.list", "NSFW.list"],
    
    # Tech company specific
    "Google.list": ["GlobalProxy.list", "NSFW.list"],
    "Microsoft.list": ["GlobalProxy.list", "NSFW.list"],
    "Apple.list": ["GlobalProxy.list", "NSFW.list"],
    "GitHub.list": ["GlobalProxy.list", "NSFW.list"],
    "Bing.list": ["Microsoft.list", "GlobalProxy.list", "NSFW.list"],
    
    # China services specific (merged Tencent)
    "Tencent.list": ["ChinaDirect.list", "GlobalProxy.list"],
    "Bilibili.list": ["ChinaDirect.list", "GlobalProxy.list"],
    "XiaoHongShu.list": ["ChinaDirect.list", "GlobalProxy.list"],
    
    # ========== Third Priority: Fallback rulesets ==========
    # These rulesets have lowest priority, will be overridden by specific rulesets
    # GlobalProxy, SocialMedia, Gaming, AI, NSFW, etc.
}

# Also standard exclusions: Remove "Direct" domains from "Proxy" lists if they appear?
# Maybe too risky. Focus on the defined map.

def is_valid_rule(line):
    """Check if rule is valid (Surge/Shadowrocket compatible)"""
    # Skip RULE-SET (should not appear in .list files)
    if line.startswith('RULE-SET'):
        return False
    
    # DOMAIN/DOMAIN-SUFFIX/DOMAIN-KEYWORD cannot have no-resolve
    # no-resolve is only for IP-CIDR/IP-CIDR6/GEOIP rules
    if line.startswith('DOMAIN') and ',no-resolve' in line:
        return False
    
    return True

def clean_rule(line):
    """Clean rule, remove invalid parameters"""
    # Remove no-resolve from DOMAIN rules (if present)
    if line.startswith('DOMAIN') and ',no-resolve' in line:
        line = line.replace(',no-resolve', '')
    return line

def load_list(filepath):
    """Loads rules from a file into a set."""
    rules = set()
    if not os.path.exists(filepath):
        return rules
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('//'):
                 # Normalize: remove comments "DOMAIN,x.com # comment"
                if '#' in line:
                    line = line.split('#')[0].strip()
                # Clean invalid rules
                line = clean_rule(line)
                # Skip invalid rules
                if is_valid_rule(line):
                    rules.add(line)
    return rules

def write_list(filepath, rules):
    """Writes sorted rules back to file, preserving existing header if present."""
    sorted_rules = sorted(list(rules))
    filename = os.path.basename(filepath)
    
    # Try to preserve existing header (detailed header generated by ruleset_merger.sh)
    existing_header = []
    
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                # Keep all comment lines as header
                if line.startswith('#') or (line.strip() == ''):
                    existing_header.append(line)
                else:
                    # First rule line, header ends
                    break
    
    # Write file
    with open(filepath, 'w', encoding='utf-8') as f:
        if existing_header and len(existing_header) > 5:
            # Has detailed header, keep it (including all comments and category markers)
            for line in existing_header:
                f.write(line)
            # Add smart_cleanup marker at end of header
            f.write(f"# [smart_cleanup.py] Deduplicated: {len(sorted_rules)} rules\n")
            f.write("\n")
        else:
            # No detailed header, use simple header
            f.write(f"# Ruleset: {filename}\n")
            f.write("# Cleaned by smart_cleanup.py\n")
            f.write(f"# Total: {len(sorted_rules)}\n")
            f.write("\n")
        
        # Write rules (no longer adding category markers, already in header)
        for rule in sorted_rules:
            f.write(rule + "\n")

def main():
    print("Starting Smart Cleanup...")
    
    # 1. Load all content into memory map
    file_content = {} # filename -> set of rules
    
    # Get all .list files
    files = glob.glob(os.path.join(RULESET_DIR, "*.list"))
    for fpath in files:
        fname = os.path.basename(fpath)
        file_content[fname] = load_list(fpath)
        
    # 2. Apply Conflict Map (Subtraction)
    for specific_name, generic_names in CONFLICT_MAP.items():
        if specific_name not in file_content:
            continue
            
        specific_rules = file_content[specific_name]
        
        for generic_name in generic_names:
            if generic_name in file_content:
                original_count = len(file_content[generic_name])
                # Subtract
                file_content[generic_name] -= specific_rules
                new_count = len(file_content[generic_name])
                
                diff = original_count - new_count
                if diff > 0:
                    print(f"Removed {diff} rules from {generic_name} (found in {specific_name})")

    # 3. Global Unique Enforcement (Optional but requested "ensure no repeats")
    # This is tricky because "who wins?". 
    # We can rely on the Conflict Map for explicit wins.
    # For others, maybe we don't care, or we just let them exist.
    # User said "Ensure ruleset and ruleset do not repeat". 
    # Let's do a simple pass: If a rule is in "Generic" lists, keep it there ONLY if not in specific?
    # We already did that.
    
    # 4. Save changed files
    for fname, rules in file_content.items():
        fpath = os.path.join(RULESET_DIR, fname)
        write_list(fpath, rules)
        
    print("Smart Cleanup Complete.")

if __name__ == "__main__":
    main()
