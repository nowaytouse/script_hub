import os
import glob

# Configuration
RULESET_DIR = os.path.join(os.path.dirname(__file__), "../ruleset/Surge(Shadowkroket)")

# Priority Definitions (Higher priority lists steal domains from Lower priority lists)
# Format: "Specific": ["Generic1", "Generic2"]
# Meaning: If a domain is in Specific, remove it from Generic1 and Generic2.
#
# ðŸ”¥ ä¼˜å…ˆçº§é¡ºåºï¼ˆä»Žé«˜åˆ°ä½Žï¼‰:
#   1. å¹¿å‘Šæ‹¦æˆªè§„åˆ™é›† (AdBlock, NSFW) - æœ€é«˜ä¼˜å…ˆçº§
#   2. ç»†åˆ†ç½‘ç«™è§„åˆ™é›† (Twitter, Netflix, Steamç­‰) - ä¸­ç­‰ä¼˜å…ˆçº§
#   3. å…œåº•è§„åˆ™é›† (GlobalProxy, GlobalMedia, SocialMediaç­‰) - æœ€ä½Žä¼˜å…ˆçº§
#
CONFLICT_MAP = {
    # ========== ç¬¬ä¸€ä¼˜å…ˆçº§: å¹¿å‘Šæ‹¦æˆª ==========
    # AdBlockä¼˜å…ˆäºŽæ‰€æœ‰å…¶ä»–è§„åˆ™é›†
    "AdBlock.list": ["GlobalProxy.list", "GlobalMedia.list", "SocialMedia.list", 
                     "Google.list", "Microsoft.list", "Apple.list",
                     "Twitter.list", "Instagram.list", "Facebook.list",
                     "YouTube.list", "Netflix.list", "Spotify.list"],
    "AdBlock_Merged.list": ["GlobalProxy.list", "GlobalMedia.list", "SocialMedia.list",
                            "Google.list", "Microsoft.list", "Apple.list"],
    
    # ========== ç¬¬äºŒä¼˜å…ˆçº§: ç»†åˆ†ç½‘ç«™è§„åˆ™é›† ==========
    # ç¤¾äº¤åª’ä½“ç»†åˆ†
    "Twitter.list": ["SocialMedia.list", "GlobalMedia.list", "GlobalProxy.list"],
    "Instagram.list": ["SocialMedia.list", "GlobalMedia.list", "GlobalProxy.list"],
    "Facebook.list": ["SocialMedia.list", "GlobalMedia.list", "GlobalProxy.list"],
    "Telegram.list": ["SocialMedia.list", "GlobalMedia.list", "GlobalProxy.list"],
    "TikTok.list": ["SocialMedia.list", "GlobalMedia.list", "GlobalProxy.list"],
    "Reddit.list": ["SocialMedia.list", "GlobalMedia.list", "GlobalProxy.list"],
    
    # æµåª’ä½“ç»†åˆ†
    "YouTube.list": ["GlobalMedia.list", "GlobalProxy.list", "Google.list"],
    "Netflix.list": ["GlobalMedia.list", "GlobalProxy.list"],
    "Spotify.list": ["GlobalMedia.list", "GlobalProxy.list"],
    "Disney.list": ["GlobalMedia.list", "GlobalProxy.list"],
    
    # æ¸¸æˆç»†åˆ†
    "Steam.list": ["Gaming.list", "GlobalProxy.list"],
    "Epic.list": ["Gaming.list", "GlobalProxy.list"],
    
    # AIç»†åˆ†
    "OpenAI.list": ["AI.list", "GlobalProxy.list"],
    "Claude.list": ["AI.list", "GlobalProxy.list"],
    
    # ç§‘æŠ€å…¬å¸ç»†åˆ†
    "Google.list": ["GlobalProxy.list"],
    "Microsoft.list": ["GlobalProxy.list"],
    "Apple.list": ["GlobalProxy.list"],
    "GitHub.list": ["GlobalProxy.list"],
    
    # NSFWç»†åˆ†ï¼ˆæˆäººå†…å®¹ï¼‰
    "NSFW.list": ["GlobalProxy.list"],
    
    # ========== ç¬¬ä¸‰ä¼˜å…ˆçº§: å…œåº•è§„åˆ™é›† ==========
    # è¿™äº›è§„åˆ™é›†ä¼˜å…ˆçº§æœ€ä½Žï¼Œä¼šè¢«ç»†åˆ†è§„åˆ™é›†è¦†ç›–
    # GlobalProxy, GlobalMedia, SocialMedia, Gaming, AI ç­‰
}

# Also standard exclusions: Remove "Direct" domains from "Proxy" lists if they appear?
# Maybe too risky. Focus on the defined map.

def is_valid_rule(line):
    """æ£€æŸ¥è§„åˆ™æ˜¯å¦åˆæ³•ï¼ˆSurge/Shadowrocket å…¼å®¹ï¼‰"""
    # è·³è¿‡ RULE-SETï¼ˆä¸åº”è¯¥å‡ºçŽ°åœ¨ .list æ–‡ä»¶ä¸­ï¼‰
    if line.startswith('RULE-SET'):
        return False
    
    # ðŸ”¥ DOMAIN/DOMAIN-SUFFIX/DOMAIN-KEYWORD ä¸èƒ½å¸¦ no-resolve
    # no-resolve åªèƒ½ç”¨äºŽ IP-CIDR/IP-CIDR6/GEOIP è§„åˆ™
    if line.startswith('DOMAIN') and ',no-resolve' in line:
        return False
    
    return True

def clean_rule(line):
    """æ¸…ç†è§„åˆ™ï¼Œç§»é™¤éžæ³•å‚æ•°"""
    # ç§»é™¤ DOMAIN è§„åˆ™ä¸­çš„ no-resolveï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰
    if line.startswith('DOMAIN') and ',no-resolve' in line:
        line = line.replace(',no-resolve', '')
    return line

def load_list(filepath):
    """Loads rules from a file into a set."""
    rules = set()
    if not os.path.exists(filepath):
        return rules
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('//'):
                 # Normalize: remove comments "DOMAIN,x.com # comment"
                if '#' in line:
                    line = line.split('#')[0].strip()
                # ðŸ”¥ æ¸…ç†éžæ³•è§„åˆ™
                line = clean_rule(line)
                # ðŸ”¥ è·³è¿‡éžæ³•è§„åˆ™
                if is_valid_rule(line):
                    rules.add(line)
    return rules

def write_list(filepath, rules):
    """Writes sorted rules back to file, preserving existing header if present."""
    sorted_rules = sorted(list(rules))
    filename = os.path.basename(filepath)
    
    # ðŸ”¥ å°è¯•ä¿ç•™åŽŸæœ‰headerï¼ˆç”±ruleset_merger.shç”Ÿæˆçš„è¯¦ç»†headerï¼‰
    existing_header = []
    
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                # ä¿ç•™æ‰€æœ‰æ³¨é‡Šè¡Œä½œä¸ºheader
                if line.startswith('#') or (line.strip() == ''):
                    existing_header.append(line)
                else:
                    # é‡åˆ°ç¬¬ä¸€ä¸ªè§„åˆ™è¡Œï¼Œheaderç»“æŸ
                    break
    
    # å†™å…¥æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        if existing_header and len(existing_header) > 5:
            # æœ‰è¯¦ç»†headerï¼Œä¿ç•™å®ƒï¼ˆåŒ…æ‹¬æ‰€æœ‰æ³¨é‡Šå’Œåˆ†ç±»æ ‡è®°ï¼‰
            for line in existing_header:
                f.write(line)
            # åœ¨headeræœ«å°¾æ·»åŠ smart_cleanupæ ‡è®°
            f.write(f"# [smart_cleanup.py] Deduplicated: {len(sorted_rules)} rules\n")
            f.write("\n")
        else:
            # æ²¡æœ‰è¯¦ç»†headerï¼Œä½¿ç”¨ç®€å•header
            f.write(f"# Ruleset: {filename}\n")
            f.write("# Cleaned by smart_cleanup.py\n")
            f.write(f"# Total: {len(sorted_rules)}\n")
            f.write("\n")
        
        # å†™å…¥è§„åˆ™ï¼ˆä¸å†æ·»åŠ åˆ†ç±»æ ‡è®°ï¼Œå› ä¸ºheaderä¸­å·²æœ‰ï¼‰
        for rule in sorted_rules:
            f.write(rule + "\n")

def main():
    print("Starting Smart Cleanup...")
    
    # 1. Load all content into memory map
    file_content = {} # filename -> set of rules
    
    # Get all .list files
    files = glob.glob(os.path.join(RULESET_DIR, "*.list"))
    for fpath in files:
        fname = os.path.basename(fpath)
        file_content[fname] = load_list(fpath)
        
    # 2. Apply Conflict Map (Subtraction)
    for specific_name, generic_names in CONFLICT_MAP.items():
        if specific_name not in file_content:
            continue
            
        specific_rules = file_content[specific_name]
        
        for generic_name in generic_names:
            if generic_name in file_content:
                original_count = len(file_content[generic_name])
                # Subtract
                file_content[generic_name] -= specific_rules
                new_count = len(file_content[generic_name])
                
                diff = original_count - new_count
                if diff > 0:
                    print(f"Removed {diff} rules from {generic_name} (found in {specific_name})")

    # 3. Global Unique Enforcement (Optional but requested "ensure no repeats")
    # This is tricky because "who wins?". 
    # We can rely on the Conflict Map for explicit wins.
    # For others, maybe we don't care, or we just let them exist.
    # User said "Ensure ruleset and ruleset do not repeat". 
    # Let's do a simple pass: If a rule is in "Generic" lists, keep it there ONLY if not in specific?
    # We already did that.
    
    # 4. Save changed files
    for fname, rules in file_content.items():
        fpath = os.path.join(RULESET_DIR, fname)
        write_list(fpath, rules)
        
    print("Smart Cleanup Complete.")

if __name__ == "__main__":
    main()
